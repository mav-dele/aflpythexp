---
title: "Applying Pythagorean Expectation to Australian rules football"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,
                      message = F, 
                      warning = F)
```

## Background

Pythagorean expectation (PE) is a sports analytics metric that estimates the number of games a team is "expected" to have based on the number of points scored relative to points scored against. In principle, PE allows us to evaluate whether a team has under or over-performed by comparing Pyhthagorean expected wins (or expected wins for short) against the number of games actually won (actual wins).

The appeal of PE is in its relative simplicity as expressed with the following formula:

$$ 
Win.percentage_{it}= \frac{points.scored_{it}^2}{points.scored_{it}^2+points.allowed_{it}^2}
$$

where $i$ is a given team at season $t$. 

This formula can simplified and generalised to the following, where $k$ is an optimal exponent which tends to differ from sport-to-sport.

$$ 
Win.percentage_{it} = \frac{1}{1+(\frac{points.allowed_{it}}{points.scored_{it}})^k} 
$$

Although originally developed for baseball, PE has been applied to American Football, soccer and basketball. In general Pythagorean expected win percentage tends to be correlated with actual win percentage. With that in mind, we take a dive into PE and how it might apply to the Australian Football League (AFL) context and apply a simple implementation of the formula.

This analysis will primarily focus on the following:

- Determining an optimal $k$ exponent for AFL;
- The extent to which expected wins correlates with actual wins;
- Exploring whether concepts of over/under-performance can be used as a predictor of next season performance; and
- Evaluating how the 2022 season played out when viewed from a PE lens. 

## Preliminaries 

This analysis relies on AFL ladder data from "AFL tables" which has lovingly been gathered and compiled in the `fitzroy` package. This contains ladder data/results dating back to 1897 and is current to the most recent completed season (2022) at the time of writing. While PE can be calculated at any stage of the season, for our analysis we focus on results at regular-season end. 

```{r Preliminaries, message=FALSE, warning=FALSE, include=FALSE}
rm(list = ls())

library(tidyverse)
library(Metrics)
library(lmtest)
library(fitzRoy)
library(wesanderson)
library(ggannotate)
library(ggrepel)
library(fst)
library(modelsummary)
library(margins)
library(knitr)
library(kableExtra)
library(texreg)
library(stargazer)
```

```{r File paths - hidden for git, include=FALSE}
project.path = "/Users/Maverick/Desktop/R projects/Pythagorean expectation"
```

```{r Load data, message=FALSE, warning=FALSE, include = FALSE}
# ladder.raw = fetch_ladder_afltables(season = 1897:2022) %>% 
#   rename_with(., .fn = tolower, .cols = everything()) 
# 
# write_fst(ladder.raw,
#           file.path(project.path,
#                     "raw_ladder_data.fst"))

ladder.raw = dir(project.path,
                 full.names = T, 
                 pattern = "^raw") %>% 
  read_fst()
```

```{r Define Pyth expectation formula, message=FALSE, warning=FALSE, include=FALSE}
# Function for calculating Pythagorean expectation 
# Default exponent based Matter of Stats analysis
calc_pyth = function(points_scored, points_conceded, k=3.87){
  return(1/(1+(points_conceded/points_scored)^k))
}
```

```{r Breakdown wins, include = F}
# As the ladder data doesn't contain a running total of wins/losses/draws, we calculate the number of wins manually based on the change in 'season.points' from round to round. 
# Calculate the breakdown of wins, losses and draws
ladder.tbl = ladder.raw %>%
  group_by(season, team) %>%
  # Calculate the change in points between rounds
  mutate(change.pts = case_when(round.number == 1~season.points, # Round 1 points are teams current points
                                TRUE~season.points-lag(season.points)),
         # If change==4, team won.
         result = case_when(change.pts == 4~1, # Win equals 4 points
                            change.pts==0~0, # Loss equals 0 points
                            TRUE~2), # Draw equals 2 points
         actual.wins = sum(result),
         #' AFL Tables does not include games played. We can't rely on the round number in season where there have been byes. 
         #' Similarly, historically teams haven't necessarily played the same number of games in a season (see 1943 St Kilda VFL season).
         #' As a work around, we check if a teams points for and against change in a given round changes, if not we consider them not to have 
         #' played. 
         change.aggregate.score = (score.for+score.against)-(lag(score.for)+lag(score.against)),
         # If aggregate changes, we say they played. 
         played.round = case_when(change.aggregate.score>0~1,
                                  is.na(change.aggregate.score)&round.number==1~1,
                                  change.aggregate.score==0~0),
         # Sum this variable to get number of games played
         games.played = sum(played.round)) %>%
  # We only need the final round of the season for this purpose. 
  filter(round.number == max(round.number)) %>%
  ungroup() %>% 
  # Drop helper variables
  select(-change.aggregate.score, - played.round)
```

## A special $k$ for a special sport: determining an optimal exponent 

In theory, the optimal value for $k$ is one that yields predictions closest to actual win percentage. In baseball, optimal values for $k$ have ranged anywhere between 1 and 2 depending on the source. Conversely, this number tends to be higher in basketball, somewhere closer to 15. This differential is said to reflect "chance" playing a greater role in baseball compared to basketball. Some approaches also calculate team specific exponents to more appropriately capture quality, however, using a global exponent tends to be reasonably accurate. 

For determining an optimal value for the AFL, we take a simple approach of iterating through season-ended data with different combinations of $k$ to allow us to compare expected win percentage for different values of $k$ against actual win percentage. We assess the accuracy of each iteration of $k$ using standard accuracy measures, namely mean absolute error (MAE) and root mean squared error (RMSE), with the view to minimise the size of errors.

```{r Pythagorean k-tester, include=FALSE}
# Pythagorean k-tester
start.tester = Sys.time() # To assess time to run (can be long depending of start/finishing values)
# Initialise a blank accuracy tibble
accuracy.tbl = list()
for(i in seq(1, 8, 0.01)){
  # Create a test table that iterates over different values of k
  test.tbl = ladder.tbl %>%
    mutate(actual.win.pct = actual.wins/games.played,
           pyth.win.pct = calc_pyth(score.for, score.against, k=i),
           pyth.wins = pyth.win.pct*games.played,
           win.diff = actual.wins - pyth.wins,
           pyth.pred.ladder = pyth.wins*4)
  
  # Bind accuracy values 
  accuracy.tbl = accuracy.tbl %>%
    bind_rows(tibble(k = i,
                     MAE = mae(test.tbl$actual.wins, test.tbl$pyth.wins),
                     RMSE = rmse(test.tbl$actual.wins, test.tbl$pyth.wins)))
}
end.tester = Sys.time()
end.tester-start.tester
```

The plot below illustrates the calculated MAE/RMSE for values of $k$ tested. We see that a $k$ of 3.9 minimises the MAE, while 3.89 minimises the RMSE. Given the close magnitude of both estimates, and the generally better interpretability of MAE, we rely on $k = 3.9$ for the remainder of this analysis. 

```{r Plot accuracy, message=FALSE, warning=FALSE}
accuracy.tbl %>% 
  pivot_longer(ends_with("e"),
               names_to = "measure") %>% 
  ggplot(aes(x = k, y = value, 
             #fill = measure,
             colour = measure
             ))+
  #geom_col()+
  #geom_line()+
  geom_point(size = 0.1)+
  facet_wrap(~measure)+
  #scale_fill_manual(values = wes_palette("GrandBudapest2"))+
  scale_colour_manual(values = wes_palette("GrandBudapest2"))+
  labs(title = "Accuracy based on k-parameter",
       y = "Accuracy",
       x = "k-exponent")+
  theme(legend.position = "none")
```

```{r Output optimal k, echo = F}
# Output the optimal exponent
optimal.k = accuracy.tbl %>% 
  summarise("Mean absolute error" = k[MAE==min(MAE)],
            "Root mean squared error" = k[RMSE==min(RMSE)])

kableExtra::kbl(optimal.k,caption = "Exponent accuracy table") %>% 
  kable_styling()
```

## Expectation is reality (actuals)

Having found our "optimal" $k$, we create a Pythagorean table and expected win percentage and expected number of wins (by multiplying percentage by games played). The below table provides a snapshot of the 2022 season displaying our calculated Pythagorean or expected wins and win percentage. 

```{r Pyth table, echo=FALSE, message=FALSE, warning=FALSE}
pyth.tbl = ladder.tbl %>% 
  mutate(actual.win.pct = actual.wins/games.played,
         pyth.win.pct = calc_pyth(score.for, score.against, k = optimal.k$`Mean absolute error`),
         pyth.wins = pyth.win.pct*games.played,
         win.diff = actual.wins - pyth.wins,
         win.pct.diff = actual.win.pct - pyth.win.pct,
         pyth.ladder.pts = pyth.wins*4) %>% 
  group_by(season) %>% 
  mutate(pyth.ladder.position = dense_rank(desc(pyth.ladder.pts))) %>% 
  ungroup() %>% 
  mutate(ladder.diff = ladder.position-pyth.ladder.position)

kbl(pyth.tbl %>% 
        filter(season == 2022) %>% 
        arrange(ladder.position) %>% 
        select(team, ladder.position, score.for, score.against, actual.win.pct, pyth.win.pct, actual.wins, pyth.wins) %>% 
        mutate(across(where(is.numeric), ~round(.x, 2))),
    caption = "Pythagorean table, 2022") %>% 
  kable_styling()
```

A simple way to assess how well expected wins correlates with actual wins is to run a basic linear model. Here there's a clear positive relationship, suggesting expected-wins explains 86 per cent of the the variation in actual-wins. 

```{r Linear model, echo=FALSE, results ='asis'} 
pyth.lm = lm(actual.win.pct~pyth.win.pct, data = pyth.tbl)
#stargazer::stargazer(list(pyth.lm), type = "html", title = "stargazer table")
texreg::htmlreg(list(pyth.lm), doctype = F,caption = "PE linear model", caption.above = T)
```

We can show this relationship visually by plotting expected win percentage against actual win percentage.

```{r Linear polot, echo = F, warning=FALSE, message=FALSE}
pyth.tbl %>% 
  ggplot(aes(x = pyth.win.pct, y = actual.win.pct, group = 1))+
  geom_point(colour = wes_palettes$Darjeeling2[1])+
  geom_smooth(method = "lm", colour = wes_palettes$Darjeeling2[2])+
  geom_abline(intercept = 0, slope = 1, size = 1, colour = "dark grey", linetype = "dashed")+
  scale_y_continuous(limits = c(0, 1))+
  scale_x_continuous(limits = c(0,1))+
  labs(title = "A clear positive correlation between expected and actual win percentage",
       x = "Expected win percentage",
       y = "Actual win percentage", 
       caption = "Dashed line is a 45 degree line.")
```

For completeness we also plot the residuals. 

```{r Plot residuals, echo =FALSE, warning=FALSE, message=FALSE}
pyth.tbl %>% 
  mutate(residual = pyth.win.pct-actual.win.pct) %>% 
  ggplot(aes(x = season, y = residual))+
  geom_point(colour = wes_palettes$Darjeeling2[1])+
  #geom_smooth(method = "lm", colour = wes_palettes$Darjeeling2[2], se = F)+
  geom_hline(yintercept = 0, colour = wes_palettes$Darjeeling2[2], linetype = "dashed")+
  #geom_hline(yintercept = 0, colour = "black", linetype = "dashed")+
  labs(title = "Residuals plot",
       y = "Residuals",
       x = "Season")
```

## Win-differential a predictor for next-season performance?

In isolation we can view the differential between a team's expected win percentage and actual win percentage as an assessment of whether they are under, over, or performing as expected. Here, under-performance refers to a situation where a team's actual wins is below its expected wins (**negative win-differential**). Conversely, a team is said to over-perform if its actual wins exceed expected wins (**positive win-differential**). Some may extrapolate from this (whether sensibly or not), that under-performance in a given season might lead to a stronger performance the following season. This obviously fails to account for the plethora of reasons a team's performance might improve/deteriorate season-to-season, but holding all other factors fixed, is a somewhat intuitive proposition. 

As a starting point, we plot a histogram of win-differentials (i.e. if actual wins are above expected meaning a team over-performs so to speak, this value will be positive). We notice on average, the win-differential is close to zero, consistent with the correlation between Pythagorean expected outcomes and actuals. At the extreme of the tails, teams have been observed to win 5.75 more games than expected (Collingwood, 1935) and on the other side of the spectrum, 4.67 games less than expected (North Melbourne, 2013).

```{r Hist win differential, echo = F, message = F}
pyth.tbl %>% 
  ggplot(aes(x = win.diff))+
  geom_histogram(aes(y = ..density..), 
                 fill = wes_palettes$Darjeeling2[1])+
  geom_density(colour = wes_palettes$Darjeeling2[2])+
  geom_vline(aes(xintercept = mean(win.diff)), colour = "grey", linetype = "dashed")+
  labs(title = "Histogram of difference in actual to expected wins",
       x = "Win differential",
       y = "Density")
```

### To what extent does win-differential affect performance in the next season?

A natural way to think about whether win-differential impacts a team's performance in the next season is assessing whether the probability of improvement (or deterioration) is affected by win-differential. In this context, improvement could be defined a variety of ways:

* Did the team improve the number of games it won or its win percentage?
* Did the team improve its ladder position?
* Did the team make finals?

While the first improvement metric is the most straightforward, the latter two are perhaps more "meaningful" improvements. In practice, an improvement in win percentage is arguably meaningless if it doesn't yield any other outcomes. However, measuring improvement in ladder position is more complex and dependent on not only a team's performance, but also the performance of the rest of the league. A "team-made-finals" measure of performance is even more meaningful, but again is more complex. Improvement to make finals is more scenario specific (i.e. a team that finishes last in a given season but under-performed aren't realistically expected to make finals, compared to an under-performed team that finished ninth). For our purposes, we focus on improvement in wins and ladder position and leave a more detailed analysis of making finals for another day. 

#### An aside about wins vs win-percentage

So far we have, perhaps erroneously, used the terms wins/win-percentage and win-differential/win-percentage-differential fairly interchangably, but acknowledge that while both concepts are closely related, there is some nuance to consider. 

The number of wins is arguably a more tangible/measurable unit of performance than percentage (i.e. a 0.025 ppt increase in win percentage is less meaningful than saying a team won $x$ more games). In effect "win-differential" and "win-percentage-differential" capture the same movements and are collinear. However, we do need to be mindful the number of games each season is not necessarily static, the number of games played in a season have historically increased, although the number of games player from one season to another stays reasonably similar (with the exception of COVID). While the relative measure of win-percentage-differential is probably more suitable, where possible we will try to frame our results with the more tangible win-differential (noting for robustness both measures are tested, with no change to the overall narrative). 

### Win differential and performance change

As a first pass, we look at how under/over-performance (as measured by win-differential) in a given season $t$ impacts performance in the next season ($t+1$). 

```{r Performance change table, include=FALSE}
performance.change.tbl = pyth.tbl %>% 
  arrange(season, team) %>% 
  group_by(team) %>% 
  mutate(next.ladder.position = lead(ladder.position),
         ladder.change = ladder.position-next.ladder.position, # Positive equals ladder improvement
         next.wins = lead(actual.wins),
         wins.change = next.wins-actual.wins,
         next.win.pct = lead(actual.win.pct), # Positive equals more wins next year
         win.pct.change = next.win.pct-actual.win.pct) %>% 
  ungroup() %>% 
  drop_na() %>% 
  mutate(improver.wins = case_when(wins.change<=0~0, # Same deteriorated
                           TRUE~1),
         improver.ladder = case_when(ladder.change<=0~0, 
                                     TRUE~1),
         era = case_when(season<1990~"VFL era",
                         TRUE~"AFL era")) # Improved
```

We plot team win-differential in season $t$ against the change in wins in $t+1$ (i.e. if a team wins more games in $t+1$ this number will be greater than zero). The position of a team in a given quadrant reflects whether they under/over-performed and whether they "improved" in terms of number of wins in the next season. For example, points in quadrant one (top-left) reflects teams that under-performed, but improved by increasing the number of wins in the next season. Conversely, points in the fourth quadrant (bottom-right) over-performed and then proceeded to deteriorate the next season. 

When we apply a line of best fit, we see an apparent inverse relationship between win-differential and next-season performance, consistent with our priori. 

```{r Win differential change in wins plot, echo = F, message = F}
performance.change.tbl %>% 
  ggplot(aes(x = win.diff, y = wins.change))+
  geom_point(aes(colour = era))+
  geom_smooth(method = "lm", colour = wes_palettes$Darjeeling2[2])+
  geom_vline(xintercept = 0, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 0, colour = "grey", linetype = "dashed")+
  scale_x_continuous(limits = c(-6, 6))+
  scale_y_continuous(limits = c(-15,15))+
  scale_colour_manual(values = c(wes_palettes$Darjeeling2[1], wes_palettes$Darjeeling2[4]),
                      name = "Playing era")+
  labs(title = "Under-performing teams tend to improve in number of wins the following season",
       y = "Change in number of wins in next season",
       x = "Win differential")+
  #' Use `ggannotate`
  geom_text(data = data.frame(x = c(-6, 2.25, -6, 2.25),
                              y = c(15, 15, -15, -15),
                              label = c("Under-performed, improved next year",
                                        "Over-performed, improved next year",
                                        "Under-performed, deteriorated next year",
                                        "Over-performed, deteriorated next year")),
            mapping = aes(x = x, y = y, label = label),
            size = 2.5, hjust = 0L, inherit.aes = FALSE)
```

We produce a similar plot but replace performance change to change in ladder position. Again we observe an apparent negative relationship.

```{r Win differential change in ladder plot, echo = F, message = F}
performance.change.tbl %>% 
  ggplot(aes(x = win.diff, y = ladder.change))+
  geom_point(aes(colour = era))+
  geom_smooth(method = "lm", colour = wes_palettes$Darjeeling2[2])+
  geom_vline(xintercept = 0, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 0, colour = "grey", linetype = "dashed")+
  scale_x_continuous(limits = c(-6, 6))+
  scale_colour_manual(values = c(wes_palettes$Darjeeling2[1], wes_palettes$Darjeeling2[4]),
                      name = "Playing era")+
  labs(title = "Under-performing teams tend to improve ladder position the following season",
       y = "Change in ladder position in next season",
       x = "Win differential")+
  #' Use `ggannotate`
  geom_text(data = data.frame(x = c(-6, 2.25, -6, 2.25),
                              y = c(15, 15, -15, -15),
                              label = c("Under-performed, improved next year", 
                                        "Over-performed, improved next year", 
                                        "Under-performed, deteriorated next year", 
                                        "Over-performed, deteriorated next year")),
            mapping = aes(x = x, y = y, label = label),
            size = 2.5, hjust = 0L, inherit.aes = FALSE)
```

### Modelling probability of improvement based on win-differential

We evaluate the extent to which win-differential might affect the probability a team improves in the next season by estimating a **logistic (logit)** regression. We separately look at improvement measured by change-in-wins and change-in-ladder position. 

#### Improvement in wins

We test two logit  specifications where improvement is measured based on change in wins:

- **(Model 1)** A simple single variable model where win-differential is the only predictor; and
- **(Model 2)** A multivariate logit where we include a team's ladder position and team name as a dummy. Intuitively, a team's previous finishing position may influence their level of improvement, for example, a team on the fringe of making finals may be more likely to perform better if they under-performed. Including a team dummy may capture any inherent characteristic a team might have (but we _shouldn't_ expect this to happen). 

Our outcome variable, "improvement", is equal 1 if a team improves (increases its number of wins) in the next season, or 0 otherwise.

Both models suggest win-differential has a statistically significant negative effect on the probability of a team improving (based on wins) in the next season. To re-iterate, this means if a team over-performs (having more actual wins than expected wins), they are less likely to improve in the following season and vice versa. 

Calculating marginal effects suggest increasing win-differential by one game reduces the probability of improvement by around 10 per cent using the simple model or around 7.4 per cent based on the multivariate specification. 

```{r Simple logit, echo = F, message = F, results='asis'}
simple.logit.tbl = performance.change.tbl %>% 
  select(win.diff, improver.wins)

simple.logit.model = glm(improver.wins~win.diff,
                   family = binomial(link = "logit"),
                   data = simple.logit.tbl)
simple.logit.model.coef = coeftest(simple.logit.model)

multiple.logit.model = glm(improver.wins~win.diff+ladder.position+team,
                           family = binomial(link = "logit"),
                           data = performance.change.tbl)
multiple.logit.model.coef = coeftest(multiple.logit.model)

texreg::htmlreg(list(simple.logit.model.coef, multiple.logit.model.coef), doctype = "html", caption = "Logit results - improvement measured by change in wins", caption.above = T)

# simple.logit.model %>% 
#         margins(variables = "win.diff")
# 
# multiple.logit.model %>% 
#         margins(variables = "win.diff")
```

We can also visualise these results by plotting the predicted probability of improvement given a certain win-differential and fitting a binomial distribution.

```{r Siple logit, echo = F, message = F}
simple.logit.tbl %>% 
  ggplot(aes(y = improver.wins, x = win.diff))+
  geom_point(colour = wes_palettes$Darjeeling2[1])+
  geom_smooth(method = "glm", method.args = list(family = "binomial"),
              colour = wes_palettes$Darjeeling2[2])+
  labs(title = "Win differential and probability of improvement",
       x = "Win differential",
       y = "Team improved next season")
```

#### Improvement in ladder position

We undertake a similar analysis with similar specifications but instead measure improvement in terms of change in ladder position. Both logit specifications yield similar statistically significant negative results. Calculating marginal effects, based on our simple model an increase in win-differential by one-game reduces the probability of improvement by around 7 per cent, with our multivariate specification suggesting a reduced probability closer to 4 per cent. This is consistent with the idea there are likely more factors affecting ladder position, in particular the relative performance of other team. 

```{r Logit ladder, echo = F, message = F, results='asis'}
simple.logit.ladder.tbl = performance.change.tbl %>% 
  select(win.diff, improver.ladder)

simple.logit.ladder.model = glm(improver.ladder~win.diff,
                   family = binomial(link = "logit"),
                   data = simple.logit.ladder.tbl)
simple.logit.ladder.model.coef = coeftest(simple.logit.ladder.model)

multi.logit.ladder.model = glm(improver.ladder~win.diff+ladder.position+team,
                   family = binomial(link = "logit"),
                   data = performance.change.tbl)
multi.logit.ladder.model.coef = coeftest(multi.logit.ladder.model)

texreg::htmlreg(list(simple.logit.ladder.model.coef, multi.logit.ladder.model.coef), doctype = "html", caption = "Logit results - improvement measured by change in ladder position", caption.above = T)

# simple.logit.ladder.model %>% 
#   margins(variables = "win.diff") 
# 
# multi.logit.ladder.model %>% 
#   margins(variables = "win.diff") 

```

The predicted probability distribution is also flatter, consistent with these results. 

```{r Logit ladder chat, echo = F, message = F}
simple.logit.ladder.tbl %>% 
  ggplot(aes(y = improver.ladder, x = win.diff))+
  geom_point(colour = wes_palettes$Darjeeling2[1])+
  geom_smooth(method = "glm", method.args = list(family = "binomial"),
              colour = wes_palettes$Darjeeling2[2])+
  labs(title = "Win differential and probability of improvement (ladder edition)",
       x = "Win differential",
       y = "Team improved next season")
```

## An analysis of the 2022 season

Finally, we review the 2022 season through the lens of PE. Looking at win-differential, Collingwood significantly over-performed (as pundits suggested), with an estimated 4 more wins than expected. Notably, Fremantle also over-performed winning almost 3 more games than expectec. Conversely, Port Adelaide strongly under-performed with 3 less wins than expected. 

```{r 2022 plot, echo = F, message = F}
pyth.tbl %>% 
  filter(season == 2022) %>%
  mutate(team = fct_reorder(team,- desc(win.diff)),
         positive = as.factor(if_else(win.diff>0, 1, 0))) %>% 
  ggplot(aes(y = team, x = win.diff, fill = positive))+
  geom_col()+
  geom_vline(xintercept = 0, colour = "black", linetype = "dashed")+
  geom_text(aes(label = round(win.diff, 1)), size = 2.5)+
  scale_fill_manual(values = wes_palette("GrandBudapest2"))+
  labs(title = "2022 win differential", 
       x = "Win-differential", 
       y = element_blank())+
  theme(legend.position = "none")
  
```

We can use calculated expected wins to crudely create an expected ladder position. Based on expected wins, the ladder shapes up quite differently.  Significantly, Collingwood would not have been expected to make finals while Port Adelaide should have expected to make finals. Richmond would also have finished fourth earning a double-finals chance. 

```{r 2022 predicted ladder, echo = F, message = F}

pyth.tbl %>% 
  filter(season == 2022) %>%
  select(team, ends_with("position")) %>% 
  pivot_longer(-team) %>% 
  group_by(team) %>% 
  mutate(actual.lad = value[name=="ladder.position"]) %>% 
  ungroup() %>% 
  mutate(team = fct_reorder(team, desc(actual.lad))) %>% 
  ggplot(aes(y = team, x = value, colour = name, group = name))+
  geom_point()+
  scale_x_continuous(breaks = seq(1, 18, 1))+
    scale_colour_manual(values = wes_palette("Darjeeling2"),
                    labels = c("Actual ladder position", "Pythagorean expected ladder position"))+
  labs(title = "2022 actual v predicted ladder", 
       x = "Ladder position", 
       y = element_blank())+
  theme(legend.position = "bottom",
        legend.title = element_blank())
```

If we reflect back on 2021, while Port Adelaide slightly over-performed and Collingwood slightly under-performed, that in of itself doesn't necessarily explain the significant shifts both teams saw in 2022. 

```{r echo = F}
kbl(performance.change.tbl %>% 
  filter(season == 2021) %>% 
    mutate(across(where(is.numeric), ~round(.x, 1))) %>% 
  select(season, team, ladder.position, actual.wins, pyth.wins, win.diff) %>% 
  arrange(-win.diff)) %>% 
  kable_styling()
```



```{r Ladder change win diff, echo = F, message = F, warning = F}
performance.change.tbl %>% 
  filter(season == 2021) %>% 
  mutate(class = case_when(win.diff<0&ladder.change>0~"Under - improved",
                              win.diff>0&ladder.change>0~"Over - improved",
                              win.diff>0&ladder.change<0~"Under - deteriorated",
                              win.diff<0&ladder.change<0~"Over - deteriorated")) %>% 
  ggplot(aes(x = win.diff, y = ladder.change, colour = class))+
  geom_point(size = 2)+
  geom_vline(xintercept = 0, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 0, colour = "grey", linetype = "dashed")+
  #geom_text(aes(label = team), size = 2.5, hjust=-1)+
  geom_text_repel(aes(label = team), size = 2.5, hjust=-1)+
  scale_x_continuous(limits = c(-3, 3))+
  scale_y_continuous(limits = c(-15, 15))+
  scale_colour_manual(values = wes_palette("GrandBudapest2"))+
  labs(title = "Win differential 2021 and change in ladder position 2022",
       x = "Win differential, 2021",
       y = "Change in ladder position (2021 to 2022)")+
  theme(legend.position = "none")+
  geom_text(data = data.frame(x = c(-2.5, 1.5, -2.5, 1.5),
                              y = c(15, 15, -15, -15),
                              label = c("Under-performed, improved next year", 
                                        "Over-performed, improved next year", 
                                        "Under-performed, deteriorated next year", 
                                        "Over-performed, deteriorated next year")),
            mapping = aes(x = x, y = y, label = label),
            size = 2.5, hjust = 0L, inherit.aes = FALSE)