---
title: "Applying Pythagorean Expectation to Australian rules football"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F,
                      message = F, 
                      warning = F)
```

## Background

Pythagorean expectation (PE) is a sports analytics metric originally developed for baseball that estimates the proportion or number of games a team is "expected" to have won as a function of points scored relative to points scored against. In principle, PE allows us to evaluate whether a team is or has under or over-performed when comparing Pyhthagorean expected wins (or expected wins for short) against the number of games the has actually won (actual wins).

The appeal of PE is in its relative simplicity as expressed with the following formula:

$$ 
Win.percentage_{it}= \frac{points.scored_{it}^2}{points.scored_{it}^2+points.allowed_{it}^2}
$$

where $i$ is a given team at season $t$. 

This formula can simplified and generalised to the following, where $k$ is an optimal exponent which tends to differ from sport-to-sport.

$$ 
Win.percentage_{it} = \frac{1}{1+(\frac{points.allowed_{it}}{points.scored_{it}})^k} 
$$

PE has been applied to American Football, soccer and basketball, with Pythagorean expected win percentage tending to be correlated with actual win percentage. With that in mind, we take a dive into PE and how it might apply to the Australian Football League (AFL) context and apply a simple implementation of the formula.

This analysis will primarily focus on the following:

- Determining an optimal $k$ exponent for AFL;
- The extent to which expected wins correlates with actual wins;
- Exploring whether concepts of over/under-performance can be used as a predictor of next season performance; and
- Evaluating how the 2022 season played out when viewed from a PE lens. 

## Preliminaries 

This analysis relies on AFL ladder data from "AFL tables" which has lovingly been gathered and compiled in the `fitzroy` package. This contains ladder data/results dating back to 1897 and is current to the most recent completed season (2022) at the time of writing. While PE can be calculated at any stage of the season, for our analysis we focus on results at regular-season end. 

```{r Preliminaries, message=FALSE, warning=FALSE, include=FALSE}
rm(list = ls())

library(tidyverse)
library(Metrics)
library(lmtest)
library(fitzRoy)
library(wesanderson)
library(ggannotate)
library(ggrepel)
library(fst)
library(modelsummary)
library(margins)
library(knitr)
library(kableExtra)
library(texreg)
library(stargazer)
```

```{r File paths - hidden for git, include=FALSE}
project.path = "/Users/Maverick/Desktop/R projects/Pythagorean expectation"
```

```{r Load data, message=FALSE, warning=FALSE, include = FALSE}
# ladder.raw = fetch_ladder_afltables(season = 1897:2022) %>% 
#   rename_with(., .fn = tolower, .cols = everything()) 
# 
# write_fst(ladder.raw,
#           file.path(project.path,
#                     "raw_ladder_data.fst"))

ladder.raw = dir(project.path,
                 full.names = T, 
                 pattern = "^raw") %>% 
  read_fst()
```

```{r Define Pyth expectation formula, message=FALSE, warning=FALSE, include=FALSE}
# Function for calculating Pythagorean expectation 
# Default exponent based Matter of Stats analysis
calc_pyth = function(points_scored, points_conceded, k=3.87){
  return(1/(1+(points_conceded/points_scored)^k))
}
```

```{r Breakdown wins, include = F}
# As the ladder data doesn't contain a running total of wins/losses/draws, we calculate the number of wins manually based on the change in 'season.points' from round to round. 
# Calculate the breakdown of wins, losses and draws
ladder.tbl = ladder.raw %>%
  group_by(season, team) %>%
  # Calculate the change in points between rounds
  mutate(change.pts = case_when(round.number == 1~season.points, # Round 1 points are teams current points
                                TRUE~season.points-lag(season.points)),
         # If change==4, team won.
         result = case_when(change.pts == 4~1, # Win equals 4 points
                            change.pts==0~0, # Loss equals 0 points
                            TRUE~2), # Draw equals 2 points
         actual.wins = sum(result),
         #' AFL Tables does not include games played. We can't rely on the round number in season where there have been byes. 
         #' Similarly, historically teams haven't necessarily played the same number of games in a season (see 1943 St Kilda VFL season).
         #' As a work around, we check if a teams points for and against change in a given round changes, if not we consider them not to have 
         #' played. 
         change.aggregate.score = (score.for+score.against)-(lag(score.for)+lag(score.against)),
         # If aggregate changes, we say they played. 
         played.round = case_when(change.aggregate.score>0~1,
                                  is.na(change.aggregate.score)&round.number==1~1,
                                  change.aggregate.score==0~0),
         # Sum this variable to get number of games played
         games.played = sum(played.round)) %>%
  # We only need the final round of the season for this purpose. 
  filter(round.number == max(round.number)) %>%
  ungroup() %>% 
  # Drop helper variables
  select(-change.aggregate.score, - played.round)
```

## A special $k$ for a special sport: determining an optimal exponent 

In theory, the optimal value for $k$ is one that yields predictions closest to actual win percentage. In baseball, optimal values for $k$ have ranged anywhere between 1 and 2 depending on the source. Conversely, this number tends to be higher in basketball, somewhere closer to 15. This differential is said to reflect "chance" playing a greater role in baseball compared to basketball.

For determining an optimal value for AFL, we take a simple approach of looping/iterating through season-ended data with different combinations of $k$ to allow us to compare expected wins for different values of $k$ against actual win percentage. We assess the accuracy of each iteration of $k$ using standard accuracy measures, namely mean absolute error (MAE) and root mean squared error (RMSE), with the view to minimise the size of errors.

```{r Pythagorean k-tester, include=FALSE}
# Pythagorean k-tester
start.tester = Sys.time() # To assess time to run (can be long depending of start/finishing values)
# Initialise a blank accuracy tibble
accuracy.tbl = list()
for(i in seq(1, 8, 0.01)){
  # Create a test table that iterates over different values of k
  test.tbl = ladder.tbl %>%
    mutate(actual.win.pct = actual.wins/games.played,
           pyth.win.pct = calc_pyth(score.for, score.against, k=i),
           pyth.wins = pyth.win.pct*games.played,
           win.diff = actual.wins - pyth.wins,
           pyth.pred.ladder = pyth.wins*4)
  
  # Bind accuracy values 
  accuracy.tbl = accuracy.tbl %>%
    bind_rows(tibble(k = i,
                     MAE = mae(test.tbl$actual.wins, test.tbl$pyth.wins),
                     RMSE = rmse(test.tbl$actual.wins, test.tbl$pyth.wins)))
}
end.tester = Sys.time()
end.tester-start.tester
```

The plot below illustrates the calculated MAE/RMSE for values of $k$ tested. We see that a $k$ of 3.9 minimises the MAE, while 3.89 minimises the RMSE. Given the close magnitude of both estimates, and the generally better interpretability, we rely on $k = 3.9$ for the remainder of this analysis. 

```{r Plot accuracy, message=FALSE, warning=FALSE}
accuracy.tbl %>% 
  pivot_longer(ends_with("e"),
               names_to = "measure") %>% 
  ggplot(aes(x = k, y = value, fill = measure))+
  geom_col()+
  #geom_line()+
  facet_wrap(~measure)+
  scale_fill_manual(values = wes_palette("GrandBudapest2"))+
  labs(title = "Accuracy based on k-parameter",
       y = "Accuracy",
       x = "k-exponent")+
  theme(legend.position = "none")
```

```{r Output optimal k, echo = F}
# Output the optimal exponent
optimal.k = accuracy.tbl %>% 
  summarise("Mean absolute error" = k[MAE==min(MAE)],
            "Root mean squared error" = k[RMSE==min(RMSE)])

kableExtra::kbl(optimal.k,caption = "Exponent accuracy table") %>% 
  kable_styling()
```

## Expectation is reality (actuals)

Having found our "optimal" $k$, we create a Pythagorean table and calculate expected wins and win percentage. The below table provides a snapshot of the 2022 season displaying our calculated Pythagorean or expected wins and win percentage. 

```{r Pyth table, echo=FALSE, message=FALSE, warning=FALSE}
pyth.tbl = ladder.tbl %>% 
  mutate(actual.win.pct = actual.wins/games.played,
         pyth.win.pct = calc_pyth(score.for, score.against, k = optimal.k$`Mean absolute error`),
         pyth.wins = pyth.win.pct*games.played,
         win.diff = actual.wins - pyth.wins,
         win.pct.diff = actual.win.pct - pyth.win.pct,
         pyth.ladder.pts = pyth.wins*4) %>% 
  group_by(season) %>% 
  mutate(pyth.ladder.position = dense_rank(desc(pyth.ladder.pts))) %>% 
  ungroup() %>% 
  mutate(ladder.diff = ladder.position-pyth.ladder.position)

kbl(pyth.tbl %>% 
        filter(season == 2022) %>% 
        arrange(ladder.position) %>% 
        select(team, ladder.position, actual.win.pct, pyth.win.pct, actual.wins, pyth.wins) %>% 
        mutate(across(where(is.numeric), ~round(.x, 2))),
    caption = "Pythagorean table, 2022") %>% 
  kable_styling()
```

As a simple check to assess how well expected wins correlates with actual wins, we run a basic linear model. Here there's a clear positive relationship, suggesting expected-wins explains 86 per cent of the the variation in actual-wins. 

```{r Linear model, echo=FALSE, results ='asis'} 
pyth.lm = lm(actual.win.pct~pyth.win.pct, data = pyth.tbl)
#stargazer::stargazer(list(pyth.lm), type = "html", title = "stargazer table")
texreg::htmlreg(list(pyth.lm), doctype = F,caption = "PE linear model")
```

We can show this relationship visually by plotting expected win percentage against actual win percentage.

```{r Linear polot, echo = F, warning=FALSE, message=FALSE}
pyth.tbl %>% 
  ggplot(aes(x = pyth.win.pct, y = actual.win.pct, group = 1))+
  geom_point(colour = wes_palettes$Darjeeling2[1])+
  geom_smooth(method = "lm", colour = wes_palettes$Darjeeling2[2])+
  geom_abline(intercept = 0, slope = 1, size = 1, colour = "dark grey", linetype = "dashed")+
  scale_y_continuous(limits = c(0, 1))+
  scale_x_continuous(limits = c(0,1))+
  labs(title = "A clear positive correlation between expected and actual win percentage",
       x = "Expected win percentage",
       y = "Actual win percentage", 
       caption = "Dashed line is a 45 degree line.")
```

For completeness we also plot the residuals. 

```{r Plot residuals, echo =FALSE, warning=FALSE, message=FALSE}
pyth.tbl %>% 
  mutate(residual = pyth.win.pct-actual.win.pct) %>% 
  ggplot(aes(x = season, y = residual))+
  geom_point(colour = wes_palettes$Darjeeling2[1])+
  #geom_smooth(method = "lm", colour = wes_palettes$Darjeeling2[2], se = F)+
  geom_hline(yintercept = 0, colour = wes_palettes$Darjeeling2[2], linetype = "dashed")+
  #geom_hline(yintercept = 0, colour = "black", linetype = "dashed")+
  labs(title = "Residuals plot",
       y = "Residuals",
       x = "Season")
```

## Win-differential a predictor for next-season performance?

In isolation we can view the differential between a team's expected win percentage and actual win percentage as an assessment of whether they are under, over, or performing as expected. Here, under-performance refers to a situation where a team's actual wins is below its expected wins (negative win-differential). Conversely, a team is said to over perform if its actual wins exceed expected wins (positive win-differential). Some may extrapolate from this (whether sensibly or not), that under-performance in a given season might lead to a stronger performance the following season. This obviously fails to account for the plethora of reasons a team's performance might improve/deteriorate season-to-season, but holding all other factors fixed, is a somewhat intuitive proposition. 

As a starting point, we plot a histogram of win-differentials (i.e. if actual wins are above expected meaning a team over-performs so to speak, this value will be positive). We notice on average, the win-differential is close to zero, consistent with the correlation between Pythagorean expected outcomes and actuals. 

```{r Hist win differential, echo = F, message = F}
pyth.tbl %>% 
  ggplot(aes(x = win.diff))+
  geom_histogram(aes(y = ..density..), 
                 fill = wes_palettes$Darjeeling2[1])+
  geom_density(colour = wes_palettes$Darjeeling2[2])+
  geom_vline(aes(xintercept = mean(win.diff)), colour = "grey", linetype = "dashed")+
  labs(title = "Histogram of difference in actual to expected wins",
       x = "Win differential",
       y = "Density")
```

### To what extent does win-differential affect performance in the next season?

A natural way to think about whether win-differential impacts a team's performance in the next season is assessing whether the probability of improvement (or deterioration) is affected by win-differential. In this context, improvement could be defined a variety of ways:

* Did the team improve the number of games it won or its win percentage?
* Did the team improve its ladder position?
* Did the team make finals?

While the first improvement metric is the most straightforward, the latter two are perhaps more "meaningful" improvements. In practice, an improvement in win percentage is arguably meaningless if it doesn't yield any other outcomes. However, measuring improvement in ladder position is more complex and dependent on not only a team's performance, but also the performance of the rest of the league. A "team-made-finals" measure of performance is more meaningful/singificant, but again is more complex. Improvement to make finals is more scenario specific (i.e. a team that finishes last in a given season but under-performed aren't realistically expected to make finals, compared to a team that finished ninth). Given we want to focus on general improvement, we analyse improvement through the lens of improving number of wins/win-percentage and ladder position. 

#### An aside about wins vs win-percentage

Thus far we have perhaps erroneously used the terms wins/win-percentage and win-differential/win-percentage-differential fairly interchangably, but acknowledge that while both concepts are fairly similar and related, there is some nuance between the two. 

The number of wins is arguably a more tangible/measurable unit of performance than percentage (i.e. a 0.025 ppt increase in win percentage is less meaningful than saying a team won $X$ more games). In effect "win-differential" and "win-percentage-differential" capture the same movements and are collinear. However, we do need to be mindful the number of games each season is not necessarily static, the number of games played in a season have historically increased, although the number of games player from one season to another stays reasonably similar (with the exception of COVID). While the relative measure of win-percentage-differential is probably more suitable, where possible we will try to frame our results with the more tangible win-differential (noting for robustness both measures are tested, with no change to the overall narrative). 

### Win differential and performance change

As a first pass we look at how under/over-performance (as measured by win-differential) in a given season $t$ impacts performance in the next season ($t+1$). 

```{r Performance change table, include=FALSE}
performance.change.tbl = pyth.tbl %>% 
  arrange(season, team) %>% 
  group_by(team) %>% 
  mutate(next.ladder.position = lead(ladder.position),
         ladder.change = ladder.position-next.ladder.position, # Positive equals ladder improvement
         next.wins = lead(actual.wins),
         wins.change = next.wins-actual.wins,
         next.win.pct = lead(actual.win.pct), # Positive equals more wins next year
         win.pct.change = next.win.pct-actual.win.pct) %>% 
  ungroup() %>% 
  drop_na() %>% 
  mutate(improver.wins = case_when(wins.change<=0~0, # Same deteriorated
                           TRUE~1),
         improver.ladder = case_when(ladder.change<=0~0, 
                                     TRUE~1)) # Improved
```

We plot team win-differential in season $t$ against the change in wins in $t+1$ (i.e. if a team wins more games in $t+1$ this number will be greater than zero). The position of a team in a given quadrant reflects whether they under/over-performed and whether they "improved" in terms of number of wins in the next season. For example, points in quadrant one (top-left) reflects teams that under-performed, but improved by increasing the number of wins in the next season. Conversely, points in the fourth quadrant (bottom-right) over-performed and then proceeded to deteriorate the next season. 

When we apply a line of best fit to all of this historical data, we see an apparent inverse relationship between win-differential and next-season performance, consistent with our priori. 

```{r Win differential change in wins plot, echo = F, message = F}
performance.change.tbl %>% 
  ggplot(aes(x = win.diff, y = wins.change))+
  geom_point(colour = wes_palettes$Darjeeling2[1])+
  geom_smooth(method = "lm", colour = wes_palettes$Darjeeling2[2])+
  geom_vline(xintercept = 0, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 0, colour = "grey", linetype = "dashed")+
  scale_x_continuous(limits = c(-6, 6))+
  scale_y_continuous(limits = c(-15,15))+
  labs(title = "Under-performing teams tend to improve in number of wins the following season",
       y = "Change in number of wins in next season",
       x = "Win differential")+
  #' Use `ggannotate`
  geom_text(data = data.frame(x = c(-6, 2.25, -6, 2.25),
                              y = c(15, 15, -15, -15),
                              label = c("Under-performed, improved next year",
                                        "Over-performed, improved next year",
                                        "Under-performed, deteriorated next year",
                                        "Over-performed, deteriorated next year")),
            mapping = aes(x = x, y = y, label = label),
            size = 2.5, hjust = 0L, inherit.aes = FALSE)
```

We produce a similar plot but replace performance change to change in ladder position. Again we observe an apparent negative relationship.

```{r Win differential change in ladder plot, echo = F, message = F}
performance.change.tbl %>% 
  ggplot(aes(x = win.diff, y = ladder.change))+
  geom_point(colour = wes_palettes$Darjeeling2[1])+
  geom_smooth(method = "lm", colour = wes_palettes$Darjeeling2[2])+
  geom_vline(xintercept = 0, colour = "grey", linetype = "dashed")+
  geom_hline(yintercept = 0, colour = "grey", linetype = "dashed")+
  scale_x_continuous(limits = c(-6, 6))+
  labs(title = "Under-performing teams tend to improve ladder position the following season",
       y = "Change in ladder position in next season",
       x = "Win differential")+
  #' Use `ggannotate`
  geom_text(data = data.frame(x = c(-6, 2.25, -6, 2.25),
                              y = c(15, 15, -15, -15),
                              label = c("Under-performed, improved next year", 
                                        "Over-performed, improved next year", 
                                        "Under-performed, deteriorated next year", 
                                        "Over-performed, deteriorated next year")),
            mapping = aes(x = x, y = y, label = label),
            size = 2.5, hjust = 0L, inherit.aes = FALSE)
```

### Modelling probability of improvement based on win-differential

A relatively simple way of evaluating the extent to which win-differential might affect the probability a team improves in the next season would be to utilise a **logistic (logit)** regression.

#### Improvement in wins

We test two logit model specifications where we measure improvement based on change in wins:

- **(Model 1)** A simple single variable model where win-differential is the only predictor; and
- **(Model 2)** A multivariate logit where we include a team's ladder position and team name as a dummy. Intuitively, a team's previous finishing position may influence their level of improvement, for example, a team on the fringe of making finals may be more likely to perform better if they under-performed. Including a team dummy may capture any inherent characteristic a team might have (but we _shouldn't_ expect this to happen). 

Our outcome variable, "improvement", is equal 1 if a team improves (increases its number of wins) in the next season, or 0 otherwise. Our prediction variable is win-differential.  

Both models suggest win-differential has a statistically significant negative effect on the probability of team improving (based on wins) in the next season. To re-iterate, this means if at team over-performs as measured by having more actual wins than expected wins, they are less likely to improve in the following season and vice versa. 

Calculating marginal effects suggest increasing win-differential by one game reduces the probability of improvement by around 10 per cent using the simple model or around 7.4 per cent based on the multivariate specification. It is also interesting to note the Gold Coast Suns dummy is negative and significant at the 5 per cent level, which may capture their underlying poor performance since entering the league. 

```{r Simple logit, echo = F, message = F, results='asis'}
simple.logit.tbl = performance.change.tbl %>% 
  select(win.diff, improver.wins)

simple.logit.model = glm(improver.wins~win.diff,
                   family = binomial(link = "logit"),
                   data = simple.logit.tbl)
simple.logit.model.coef = coeftest(simple.logit.model)

multiple.logit.model = glm(improver.wins~win.diff+ladder.position+team,
                           family = binomial(link = "logit"),
                           data = performance.change.tbl)
multiple.logit.model.coef = coeftest(multiple.logit.model)

texreg::htmlreg(list(simple.logit.model.coef, multiple.logit.model.coef), doctype = "html", caption = "Logit results - improvement measured by change in wins", caption.above = T)

# simple.logit.model %>% 
#         margins(variables = "win.diff")
# 
# multiple.logit.model %>% 
#         margins(variables = "win.diff")
```

We can also visualise these results by plotting the predicted probability of improvement given a certain win-differential and fitting a binomial fitted line.

```{r Siple logit, echo = F, message = F}
simple.logit.tbl %>% 
  ggplot(aes(y = improver.wins, x = win.diff))+
  geom_point(colour = wes_palettes$Darjeeling2[1])+
  geom_smooth(method = "glm", method.args = list(family = "binomial"),
              colour = wes_palettes$Darjeeling2[2])+
  labs(title = "Win differential and probability of improvement",
       x = "Win differential",
       y = "Team improved next season")
```

#### Improvement in ladder position

We undertake a similar analysis with similar specifications but instead measure improvement in terms of change in ladder position. Both logit specifications yield similar statistically significant negative results. Calculating marginal effects, based on our simple model an increase in win-differential by one-game reduces the probability of improvement by around 7 per cent, with our multivariate specification suggesting a reduced probability closer to 4 per cent. This is consistent with the idea there are likely more factors affecting ladder position, in particular the relative performance of other team. 

```{r Logit ladder, echo = F, message = F, results='asis'}
simple.logit.ladder.tbl = performance.change.tbl %>% 
  select(win.diff, improver.ladder)

simple.logit.ladder.model = glm(improver.ladder~win.diff,
                   family = binomial(link = "logit"),
                   data = simple.logit.ladder.tbl)
simple.logit.ladder.model.coef = coeftest(simple.logit.ladder.model)

multi.logit.ladder.model = glm(improver.ladder~win.diff+ladder.position+team,
                   family = binomial(link = "logit"),
                   data = performance.change.tbl)
multi.logit.ladder.model.coef = coeftest(multi.logit.ladder.model)

texreg::htmlreg(list(simple.logit.ladder.model.coef, multi.logit.ladder.model.coef), doctype = "html", caption = "Logit results - improvement measured by change in ladder position", caption.above = T)

# simple.logit.ladder.model %>% 
#   margins(variables = "win.diff") 
# 
# multi.logit.ladder.model %>% 
#   margins(variables = "win.diff") 

```

The predicted probability distribution is also flatter, consistent with these results. 

```{r Logit ladder chat, echo = F, message = F}
simple.logit.ladder.tbl %>% 
  ggplot(aes(y = improver.ladder, x = win.diff))+
  geom_point(colour = wes_palettes$Darjeeling2[1])+
  geom_smooth(method = "glm", method.args = list(family = "binomial"),
              colour = wes_palettes$Darjeeling2[2])+
  labs(title = "Win differential and probability of improvement (ladder edition)",
       x = "Win differential",
       y = "Team improved next season")
```

## An analysis of the 2022 season

Finally, we review the 2022 season through the lens of PE. As many pundits had assessed, Collingwood significantly over-performed, with an estimated 4 more wins than expected. Conversely, Port Adelaide strongly underperformed with 3 less wins than expected. 